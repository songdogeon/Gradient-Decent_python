# Gradient-Decent_python
경사 하강법 구현 및 학습

### 경사하강법(Gradient Decent)

##### 볼록함수(convex Function)
- 어떤 지점에서 시작해도 최소값에 도달 할 수 있음
##### 비볼록 함수(Non-Convex Function)
- 시작 위치에 따라 최적값에 도달 할 수 있음

##### 경사하강법 과정
- 한 단계마다 미분값에 따라 이동하는 방향을 결정함
- f(x) 의 값이 변하지 않을때 까지 반복함
- 미분값이 0인 지점을 찾는 방법

##### 전역 최적값, 지역 최적값
- 초기값에 따라 global minimun이 될수도 있고, local minimum이 될 수도 있음

##### 경자하강법 구현
- 경사하강법 구현
- 경사하강을 진행 하는중, 최솟값에 다다르면 종료하는 코드 구현
- step_num(반복횟수) 전에 발견하여 멈추는 경우도 발생함

##### 학습률(learning rate)
- 학습률은 적절히 지정해야함
- 너무 클 경우 발산하게됨
- 너무 작을 경우 학습을 많이해야 함

##### 안장점(saddle point)
- 기울기가 0이지만 극값이 되지않는 경우
- 경사 하강법은 안장점에서 벗어나지 못함
- 학습률 또는 초기값을 조절 해야함
